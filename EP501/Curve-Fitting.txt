CURVE FITTING ================================================================

Notation:
[some_variable]^T : matrix transpose of [some_variable]

A polynomial of order 'n' can be fit exactly to 'n+1' data points. Fitting 
higher order polynomials (quadratic or greater) to data is still a linear 
problem in terms of the unknown polynomial coefficients a,b,c, etc.

Improvement on book section about curve-fitting of the higher-order polynomials:

Second look at the linear fit problem:

f(x(i)) = a + bx(i) -> f(x(i)) is a single data point. can represent as:

fi = x(i)^T * a where x(i) = [1 x(i)] and a^T = [a b]

For the entire set of data points, f = Xa, where f^T = [f1 f2 f3 ... fn],

        [x1 x2 x3 ... xn]
X^T =   [1  2  3  ... n ], and a^T is still [a b].

because f(x) (the function generating our data points) is a linear function of
[a b]^T,

f_i = [df/da df/db][a b]^T (partial derivative!!!)

            [df/db_1  df/db_2  df/db_3  ... df/db_n]
then f =    [df/da_1  df/da_2  df/da_3  ... df/da_n]^T [a b]^T

(still partial derivatives!!!)

this matrix of partial derivatives is the JACOBIAN matrix J

Vector form of error:

e = {e_i} = y (measured data points) - f (function that you're curve-fitting at
corresponding values of the independent variable) = y - Ja (y,a are column 
vectors)

Least squares method: minimize ∑(e_i)^2 -> minimize {e^T e}
                        = min{[y-Ja]^T [y-Ja]}

(J^T J)^-1 J^T -> least-squares inverse, or Moore-Penrose inverse

benefit of doing this is that it generalizes to ANY polynomial, just write the 
Jacobian by inspection:

     | 1  x_1  (x_1)^2  (x_1)^3  ... |
J =  | 1  ...    ...      ...    ... |
     | 1  x_n  (x_n)^2  (x_n)^3  ... |

for a matrix equation Ma = y, M is equivalent to this Moore-Penrose inverse.

10/15/19

Talked about least-squares fitting for overdetermined problem last time

When testing an algorithm that will be used on real data, want to test on noisy
data first. MATLAB has various features for dealing with/generating random 
numbers:
- rand(), randi(): uniformly distributed random numbers (decimal/integer) 
  (equal probability of random number occurring in any one of many specific 
  ranges)
- histogram(): plot that shows occurrence rates of data
- randn(): normally-distributed (Gaussian distribution) decimal data (0<data<1);
    - note: data only really converges to normal distributions with many many 
      data points. 20 is not enough (class grades, as an example)
    - can specify mean and standard deviation with randn(). take your random 
      dataset generated by randn(), multiply by the chosen standard dev., and 
      add the mean: x_std_dev .* randn_noise_data + x_mean
    - can skew the bell curve to one side; any prob. & stats. book will have a 
      section on transforming sets of random data to have an asymmetric 'shoulder'
    - MATLAB has a whole toolbox for probability and statistics

Least-squares is INCREDIBLY robust; can use on incredibly noisy data and get 
decent results.

When testing software that fits curves to data, always plot actual true values 
of function (sans noise) on the same plot as the noisy values. This lets you 
estimate how well your experimental results will mirror the theoretical value. 
If your experimental data is sparse, your curve-fit will probably be off from 
the theoretical value. This method can tell you how many measurements you will 
need to take in order to have a reasonable estimate of the accuracy of your 
curve-fit. If you are designing an experiment, you will need to perform this 
sort of analysis before performing said experiment.

There are version of least-squares that take into account differences in known 
error for individual measurements (not every single measurement will have the 
same error). These are known as 'weighted least-squares' problems. Implemented 
as a(J^T W J) = (J^T) W y, where W is the weight matrix, and is diagonal.

Least-squares inverse is generally reserved for overdetermined problems. Don't 
use if you don't have enough data points; reformulate your problem or obtain 
more data points.

MATLAB will let you do a least-squares inverse by using the '\' operator.

Setting up Jacobian:
- depends on order of polynomial that you're fitting data to (line, quadratic, 
  cubic, etc.) (duplicate note; see lines 38-42 above)
- To avoid calculating an inverse, rearrange Moore-Penrose inverse 
  ((J^T J)^-1 J^T) so that inverse disappears: a(J^T J) = (J^T)y. Then solve by
  elimination.

NOTE: Can force any matrix/vector to become a column vector by calling 
vec_name(:). Represents that n x m matrix as a (n x m) x 1 column vector; 
essentially represents along same lines as column-major layout in memory, 
displaying by linear index

Definition/explanation of some terminology used in the book:
- Lagrange polynomials
  - given 2 points (a,f(a)) & (b,f(b))
  - P1(x) = (x-a)/(a-b)*f(a) + (x-b)/(b-a)*f(b) (1 for first order); 
when evaluating at a, negates 'a' term, and vice versa
  - P2(x) = (x-b)(x-c)/(a-c)(a-b)*f(a) + (x-a)(x-c)/(b-c)(b-a)*f(b)
            + (x-a)(x-b)/(c-a)(c-b)*f(c) (2nd order)
  - General form: nth degree polynomial passing through n+1 data points
                n+1  ∏(i≠j) (x - x_i)
      Pn(x) =    ∑  -------------------- f(x_j)
                j=1  ∏(i≠j) (x_j - x_i)
  - Lagrange polynomials can be constructed 'sequentially' (can construct a 2nd
    order polynomial from an extant 1st degree polynomial) by writing in a 
    'divided difference' format:

                f(b)*(x-a) - f(a)*(x-b)
      P1(x) = -------------------------
                        (b - a)

    When not using the divided difference notation, if constructing a higher 
    order Lagrange polynomial, have to go through again and reconstruct every 
    time. Instead, can directly construct higher-order Lagrange polynomials 
    through lower-order divided difference polynomials:
    
                 f_(i+1)^(n-1)*(x-x_i) - f_i^(n-1)*(x-x_i+n)
      f_i^(n) = ---------------------------------------------
                                (x_(i+n) - x_i)
   
    where the superscript in parentheses is the order of the polynomial. Then, 
    an nth-order polynomal can be constructed using Neville's Algorithm:

      f(x) = f_i^(0) + f_i^(1)*(x-x_i) + f_i^(2)*(x-x_i)*(x-x_i+1) + ... +
                f_i^(n)*(x-x_(i+n-1))*(x-x_(i+n-2))*...*(x-x_i)

    - The above method uses divided differences. Can also be done using finite 
      differences; three types: forward, backward, and centered difference 
      polynomials.
    - forward difference: ∆f_i = (f_i+1 - f_i)
    - backward difference: ∆f_i = (f_i - f_i-1)
    - centered difference: have to interpolate. see Integration/Diff Notes
